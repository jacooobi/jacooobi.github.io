
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Neural networks - a nervous shock</title>
    <meta name="description" content="">

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="../favicon.ico">

    <link rel="stylesheet" type="text/css" href="../assets/css/screen.css?v=0e4b7c56c4">
    <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400">

    <link rel="canonical" href="index.html">
    <meta name="referrer" content="origin">
    
    <meta property="og:site_name" content="ML Odyssey">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Neural networks - a nervous shock">
    <meta property="og:description" content="Until some time ago, there was a list of programming and computer science related terms that used to blow my mind. I'd think of metaprogramming or functional programming concepts like immutability and monads and assume that they are massively complex mechanisms involved under the hood. Because these are quite odd">
    <meta property="og:url" content="http://localhost:2368/neural-networks/">
    <meta property="og:image" content="http://localhost:2368/content/images/2016/06/network.jpg">
    <meta property="article:published_time" content="2016-04-08T22:16:56.456Z">
    <meta property="article:modified_time" content="2016-06-19T16:28:54.449Z">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Neural networks - a nervous shock">
    <meta name="twitter:description" content="Until some time ago, there was a list of programming and computer science related terms that used to blow my mind. I'd think of metaprogramming or functional programming concepts like immutability and monads and assume that they are massively complex mechanisms involved under the hood. Because these are quite odd">
    <meta name="twitter:url" content="http://localhost:2368/neural-networks/">
    <meta name="twitter:image:src" content="http://localhost:2368/content/images/2016/06/network.jpg">
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "ML Odyssey",
    "author": {
        "@type": "Person",
        "name": "Jacek Kubiak",
        "image": "http://localhost:2368/content/images/2016/03/photo-1.jpg",
        "url": "http://localhost:2368/author/jacek/",
        "description": "I&#x27;m a Computer Science student interested in machine learning and data science. I used to make web apps with Ruby &amp; JS at netguru.co ."
    },
    "headline": "Neural networks - a nervous shock",
    "url": "http://localhost:2368/neural-networks/",
    "datePublished": "2016-04-08T22:16:56.456Z",
    "dateModified": "2016-06-19T16:28:54.449Z",
    "image": "http://localhost:2368/content/images/2016/06/network.jpg",
    "description": "Until some time ago, there was a list of programming and computer science related terms that used to blow my mind. I&#x27;d think of metaprogramming or functional programming concepts like immutability and monads and assume that they are massively complex mechanisms involved under the hood. Because these are quite odd"
}
    </script>

    <meta name="generator" content="Ghost 0.7">
    <link rel="alternate" type="application/rss+xml" title="ML Odyssey" href="../rss/index.html">
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-76056837-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body class="post-template nav-closed">

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="index.html#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
            <li class="nav-home" role="presentation"><a href="../">Home</a></li>
    </ul>
    <a class="subscribe-button icon-feed" href="../rss/index.rss">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        


<header class="main-header post-head " style="background-image: url(../content/images/2016/06/network.jpg)">
    <nav class="main-nav overlay clearfix">
        
            <a class="menu-button icon-menu" href="index.html#"><span class="word">Menu</span></a>
    </nav>
</header>

<main class="content" role="main">
    <article class="post">

        <header class="post-header">
            <h1 class="post-title">Neural networks - a nervous shock</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2016-04-09">09 April 2016</time> 
            </section>
        </header>

        <section class="post-content">
            <p>Until some time ago, there was a list of programming and computer science related terms that used to blow my mind. I'd think of metaprogramming or functional programming concepts like immutability and monads and assume that they are massively complex mechanisms involved under the hood. Because these are quite odd sounding words you tend to give them more respect and sort of surround them with a mysterious vibe. If you share the same point of view and occasionally learn new things, you may often feel dissapointed afterwards. "Is that it?" - you may think. Computer science is both very complex and simple. And on top of it, it's extremely difficult to create simple things in programming. Needless to say, when I first heard about neural networks I started imagining this abstruse design that's supposed to mimic brain functions and was hoping for more than ever.</p>

<p>So as you probably now know, instead of a mind bending solution, I was presented with a simple but robust and well-thought design. Neural networks is an old concept, the first computational model was introduced in 1943 and was a topic of research since then. What is interesting, is that it never really took off until recently when deep learning - neural network with subnetworks in the hidden layers - was invented. The are two reasons why NN were not as popular. First of all, to produce significant results, you need a lot of computational power and even though the CPUs get exponentially faster every few years, the main power boost was provided by the recent growth of GPUs which currently totally outperform CPUs. The second reason is that NN are hard to train, it takes a lot of time to apply proper weights and many times we get little to no feedback whether we are going in a good direction (when choosing network structure or tweaking the number of layers).</p>

<p>Yes, I'm aware I didn't explain the terms above. Let's start from the very beginning. This is a very simple neural network.</p>

<p><img src="../content/images/2016/06/IMAG0490-3.jpg" alt="'simple neuron with inputs'"></p>

<p>What you see is a perceptron which has two inputs and produces a single output. Inputs are binary, so you can give either zero or one. On each edge, there is a weight, value which gets multiplied by our input. Finally, the perceptron holds a threshold, a number which determines the output. We take the sum of inputs multiplied by weights and compare this sum with the threshold and if it is greater then the perceptron returns 1, if less or equal we get 0. We can model a simple logic OR function using a single perceptron and two inputs, just like you can see on the picture below.</p>

<p><img src="../content/images/2016/06/IMAG0489-2.jpg" alt="'logic OR function'"></p>

<p>Now that's cool but useless. The same thing we could do using logistic regression. What neural networks really excel at, are (mostly) classification problems with large number of variables. Logistic regression started to actually struggle when we introduced multiple variables. It still works, but it takes massive amount of time for gradient descent to come up with a proper polynomial parameters to fit the training data. In a common programming problem, that is image recognition, we use each pixel of an image as input. Given a small image, consisting of just 50x50 pixels, each in rgb(three values instead of one, like in greyscale, ranging from 0 to 255) we will end up with 7500 variables. A neural network will have no problem coping with this.</p>

<p>Nowadays, we use sigmoid neurons rather than perceptrons. What is the difference? They look the same, but with sigmoid neurons we can work using a different input than binary, that is any value between 0 and 1. There is also a bias value, which is almost the same as threshold, but it has an opposite sign. So we actually end up summing all the weights multiplied by inputs and finally, add the bias. This total value is passed as an argument to the sigmoid function, which is our neuron output and returns a values ranging from 0 to 1. I've talked about sigmoid functions a bit more in the previous post.</p>

<p><img src="../content/images/2016/06/sigmoid.png" alt="cos z sigmoidem"></p>

<p>Neural networks have layers. The network used to compute an OR function had just two layers. Input and output. If we were to introduce another layer, computes a some sort of intermediate result, we would add it in between input and output. This type of layers are called hidden layers. Without them, it would be impossible to use NN for anything more complicated than simple linear functions. This is where the real power of networks resides. In Deep Learning, we go even further. Within the hidden layers, there are other independant neural networks which sometimes have other networks nested inside. This technique was used in Alpha Go, the famous computer who beat Lee Sedol, one of the best players of the decade, in the game called Go.</p>

<p><img src="../content/images/2016/06/networks.png" alt="cos z complex network"></p>

<p>I casually mentioned image recognition as one of the prominent fields where Neural Networks are applied. In fact, NN are everywhere. Character recognition, stock market prediction, robotics, you name it. They can also be a building block in reinforcement learning, a third type of machine learning, after supervised and unsupervised learning.</p>

<p>Reinforcement learning differs from supervised learning as there aren't any sets of correct input-output pairs provided. Instead, it is concentrated on a constant reevaluation of an environment, where each decision results in a reward if it was good and a penalty - in case it was incorrect. This is like training a dog to play fetch, if it chases the ball and returns it, you might give it a snack. It's worth noting that in reinforcement learning we aren't necessarily going to finish executing the program, it is supposed to run looped and go through a constant action-feedback-(reward|penalty) cycle. This type of learning is a type of problem, where an agent has to find the best possible action in his current state.</p>

<p>Areas where RL can be applied are wide but I wanted to show you something really cool. Turns out Google's research team DeepMind developed an <a href="http://www.wired.co.uk/article/google-deepmind-atari">algorithm</a> which   taught itself to play almost 50 video games including Space Invaders and Pong. There is also a guy, who used RL to play <a href="http://sarvagyavaish.github.io/FlappyBirdRL/">Flappy Bird</a>.</p>

<p>Neural networks, thanks to deep learning, are gaining more exposure nowadays. They offer an easy way to start playing with machine learning. Even though they are a simple concept, with a proper structure and adjusted weights, you might be able to produce suprisingly good results like an algorithm which given an image is able to tell what number the image shows, with over 95% accuracy! (More on that in the Michael Nielsen's free <a href="http://neuralnetworksanddeeplearning.com/chap1.html">book</a>.</p>

<p>Oh and one more thing, since I don't post often, you might be interested a <a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">great video series</a> which presents different machine learning papers in a very accessible way, all that in under 2 minutes!</p>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="../author/jacek/" style="background-image: url(../content/images/2016/03/photo-1.jpg)"><span class="hidden">Jacek Kubiak's Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href="../author/jacek/">Jacek Kubiak</a></h4>

                    <p>I'm a Computer Science student interested in machine learning and data science. I used to make web apps with Ruby &amp; JS at netguru.co .</p>
                <div class="author-meta">
                    <span class="author-location icon-location">Poznan, Poland</span>
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=Neural%20networks%20-%20a%20nervous%20shock&amp;url=http://localhost:2368/neural-networks/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/neural-networks/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://localhost:2368/neural-networks/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>

        </footer>

    </article>
</main>

<aside class="read-next">
    <a class="read-next-story prev no-cover" href="../supervised-and-unsupervised/">
        <section class="post">
            <h2>On supervised and unsupervised learning. Gradient descent demystified.</h2>
            <p>Machine learning is a broad field with plenty of applications. In order to get a better overview of them,…</p>
        </section>
    </a>
</aside>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



        <footer class="site-footer clearfix">
            <section class="copyright"><a href="../">ML Odyssey</a> © 2016</section>
            <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
        </footer>

    </div>

    <script type="text/javascript" src="https://code.jquery.com/jquery-1.12.0.min.js"></script>
    

    <script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=0e4b7c56c4"></script>
    <script type="text/javascript" src="../assets/js/index.js?v=0e4b7c56c4"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
</body>
